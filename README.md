# NLP-study

NLP에 관한 논문 정리와 코드 구현을 저장하는 곳입니다.


|Paper|Link|Review|Code|
|---|:---:|:---:|:---:|
|Attention Is All You Need|[Link](https://arxiv.org/pdf/1706.03762.pdf)|||
|KLUE: Korean Language Understanding Evaluation|[Link](https://arxiv.org/pdf/2105.09680.pdf)|||
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|[Link](https://arxiv.org/pdf/1810.04805.pdf)|||
|RoBERTa: A Robustly Optimized BERT Pretraining Approach|[Link](https://arxiv.org/pdf/1907.11692.pdf)|||
|ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators|[Link](https://arxiv.org/pdf/2003.10555.pdf)|||
|An Improved Baseline for Sentence-level Relation Extraction|[Link](https://arxiv.org/pdf/2102.01373.pdf)|||
|ALBERT: A Lite BERT for Self-supervised Learning of Language Representations|[Link](https://arxiv.org/pdf/1909.11942.pdf)|||
|BART: Denoising Sequence-to-Sequence Pre-training forNatural Language Generation, Translation, and Comprehension|[Link](https://arxiv.org/pdf/1910.13461.pdf)|||
|Don't Stop Pretraining: Adapt Language Models to Domains and Tasks|[Link](https://arxiv.org/pdf/2004.10964.pdf)|||
|EDA: Easy Data Augmentation Techniques for Boosting Performanceon Text Classification Tasks|[Link](https://arxiv.org/pdf/1901.11196.pdf)|||
|FEW-SHOT LEARNING WITH GRAPH NEURAL NETWORKS|[Link](https://arxiv.org/pdf/1711.04043v3.pdf)|||
|Active Learning: Problem Settings and Recent Developments|[Link](https://arxiv.org/pdf/2012.04225.pdf)|||
